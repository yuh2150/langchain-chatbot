{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, Literal , List\n",
    "from pydantic import BaseModel, Field , field_validator, ValidationInfo, model_validator\n",
    "from langgraph.graph.message import AnyMessage , add_messages\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "import sys\n",
    "import os\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\")))\n",
    "\n",
    "from booking_agent.api.booking import BookingAPI\n",
    "from booking_agent.api.geoCoding import GeoCodingAPI\n",
    "from booking_agent.api.getKey import OAuthClient\n",
    "from booking_agent.api.getQuotes import QuotesAPI\n",
    "from booking_agent.api.is_Airport import IsAirport\n",
    "jupiterAPI = os.getenv('JUPITER_API')\n",
    "quoteAPI = str(jupiterAPI) + \"/demand/v1/quotes\"\n",
    "bookingsAPI  = str(jupiterAPI) + '/demand/v1/bookings'\n",
    "\n",
    "class BookingCarDetails(BaseModel):\n",
    "    \"\"\"Details for the bookings car details\"\"\"\n",
    "    name: str = Field(\n",
    "        ...,\n",
    "        description=\"The name of the person booking the ride. Do not autofill if not provided\",\n",
    "    )\n",
    "    number_phone: str = Field(\n",
    "        ...,\n",
    "        description=\"The phone number of the user. Do not autofill if not provided\",\n",
    "    )\n",
    "    pick_up_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The location where the user will be picked up. This can be a full address or a specific location name. Do not autofill if not provided\",\n",
    "    )\n",
    "    destination_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The destination location for the ride. This can be a full address or a specific location name. Do not autofill if not provided\"\n",
    "    )\n",
    "    pick_up_time: str = Field(\n",
    "        ...,\n",
    "        description=\"The time the user intends to be picked up. No format keeps the text related to time. Do not autofill if not provided\"\n",
    "    )\n",
    "    flight_code: str = Field(\n",
    "        # default= 'None',\n",
    "        ...,\n",
    "        description=\"Flight numbers, consisting of letters and numbers, usually start with the airline code (e.g. VN123, SQ318).\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('pick_up_location')\n",
    "    @classmethod\n",
    "    def validate_pickup(cls, value:str):\n",
    "        geoCodingAPI = GeoCodingAPI()\n",
    "        if value == '':\n",
    "            return ''\n",
    "        else :\n",
    "            geoCoding_pickup = geoCodingAPI.get_geocoding(value)\n",
    "            if geoCoding_pickup[\"status\"] == \"OK\" :\n",
    "                return geoCoding_pickup['results'][0]['formatted_address']\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid pick-up location: {value}\")\n",
    "            \n",
    "    @field_validator('destination_location')\n",
    "    @classmethod\n",
    "    def validate_destination(cls, value : str, info: ValidationInfo):\n",
    "        geoCodingAPI = GeoCodingAPI()\n",
    "        \n",
    "        # print (geoCoding_destination['results'][0]['formatted_address'])\n",
    "        if value == '':\n",
    "            return ''\n",
    "        else :\n",
    "            geoCoding_destination = geoCodingAPI.get_geocoding(value)\n",
    "            if geoCoding_destination[\"status\"] == \"OK\":\n",
    "                if geoCoding_destination['results'][0]['formatted_address'] == info.data['pick_up_location']:\n",
    "                    raise ValueError(f\"Invalid destination location: {value}\")\n",
    "                else:\n",
    "                    return geoCoding_destination['results'][0]['formatted_address']\n",
    "            else:\n",
    "            \n",
    "                raise ValueError(f\"Invalid destination location: {value}\")\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_flight_code_if_airport(self):\n",
    "        geoCodingAPI = GeoCodingAPI()\n",
    "        API_Airport = IsAirport(base_url=jupiterAPI + '/v2/distance/airport')\n",
    "\n",
    "        if self.pick_up_location:\n",
    "            geoCoding_pickup = geoCodingAPI.get_geocoding(self.pick_up_location)\n",
    "            if geoCoding_pickup[\"status\"] == \"OK\":\n",
    "                pick_up_lat = geoCoding_pickup['results'][0]['geometry']['location']['lat']\n",
    "                pick_up_lng = geoCoding_pickup['results'][0]['geometry']['location']['lng']\n",
    "                \n",
    "                is_Airport = API_Airport.is_Airport(pick_up_lat, pick_up_lng)\n",
    "\n",
    "                if is_Airport[0] == False:  # Nếu là sân bay\n",
    "                    self.flight_code = 'No Request'\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm.with_structured_output(BookingCarDetails)\n",
    "# reponse = chain.invoke(\"i want cancel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse = chain.invoke(\"i want to book a car to 271 Nguyen Van Linh, Da Nang from 460 Tran Dai Nghia, Da Nang at 9 tomorrow, my name is Huy call me 0917181880 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookingCarDetails(name='Huy', number_phone='0917181880', pick_up_location='460 Đ. Trần Đại Nghĩa, Hoà Hải, Điện Bàn, Quảng Nam 550000, Vietnam', destination_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='9 tomorrow', flight_code='No Request')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = {\n",
    "    \"name\": \"\",\n",
    "    \"number_phone\": \"\",\n",
    "    \"pick_up_location\": \"\",\n",
    "    \"destination_location\": \"\",\n",
    "    \"pick_up_time\": \"\",\n",
    "    \"flight_code\": \"\"\n",
    "}\n",
    "temp_data.update({\"pick_up_location\": \"Danang Airport\"})  # Chỉ cập nhật field cần thay đổi\n",
    "temp_detail = BookingCarDetails(**temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_detail.pick_up_location = \"Dannang Airport\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookingCarDetails(name='', number_phone='', pick_up_location='Danang International Airport, Đ. Nguyễn Văn Linh, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='', pick_up_time='', flight_code='')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_what_is_empty(user_peronal_details):\n",
    "    ask_for = []\n",
    "    # Check if fields are empty\n",
    "    for field, value in user_peronal_details.model_dump().items():\n",
    "        if value in [None, \"\", 0]:  # You can add other 'empty' conditions as per your requirements\n",
    "            print(f\"Field '{field}' is empty.\")\n",
    "            ask_for.append(f'{field}')\n",
    "    return ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'pick_up_location' is empty.\n",
      "Field 'destination_location' is empty.\n",
      "Field 'pick_up_time' is empty.\n",
      "Field 'flight_code' is empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pick_up_location', 'destination_location', 'pick_up_time', 'flight_code']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for = check_what_is_empty(reponse)\n",
    "ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_non_empty_details(current_details: BookingCarDetails, new_details: BookingCarDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.model_dump().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.model_copy(update=non_empty_details)\n",
    "    return updated_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_details = BookingCarDetails(name=\"\",number_phone=\"\",pick_up_location=\"\",destination_location=\"\", pick_up_time=\"\",flight_code=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_details = add_non_empty_details(booking_details ,reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookingCarDetails(name='Huy', number_phone='0917181880', pick_up_location='', destination_location='', pick_up_time='', flight_code='')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booking_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'pick_up_location' is empty.\n",
      "Field 'destination_location' is empty.\n",
      "Field 'pick_up_time' is empty.\n",
      "Field 'flight_code' is empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pick_up_location', 'destination_location', 'pick_up_time', 'flight_code']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for = check_what_is_empty(booking_details)\n",
    "ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_info(ask_list:list):\n",
    "    # prompt template 1\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Below is are some things to ask the user for in a coversation way. you should only ask one question at a time even if you don't get all the info \\\n",
    "        don't ask as a list! Don't greet the user! Don't say Hi.Explain you need to get some info. If the ask_for list is empty then thank them and ask how you can help them \\n\\n \\\n",
    "        ### ask_for list: {ask_for}\"\n",
    "    )\n",
    "    # info_gathering_chain\n",
    "    info_gathering_chain = first_prompt | llm | StrOutputParser()\n",
    "    ai_chat = info_gathering_chain.invoke({\"ask_for\": ask_list})\n",
    "    print(first_prompt)\n",
    "    return ai_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_response(text_input, user_details, ask_for):\n",
    "    chain = llm.with_structured_output(BookingCarDetails)\n",
    "    if ask_for:  # Kiểm tra xem ask_for có phần tử không\n",
    "        messages = f\"{ask_for[0]} : {text_input}\"\n",
    "    else:\n",
    "        messages = text_input\n",
    "    print(messages)\n",
    "    res = chain.invoke(messages)\n",
    "    # add filtered info to the\n",
    "    user_details = add_non_empty_details(user_details,res)\n",
    "    print(user_details)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    return user_details, ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'booking_details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbooking_details\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'booking_details' is not defined"
     ]
    }
   ],
   "source": [
    "booking_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'number_phone']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Below is are some things to ask the user for in a coversation way. you should only ask one question at a time even if you don't get all the info         don't ask as a list! Don't greet the user! Don't say Hi.Explain you need to get some info. If the ask_for list is empty then thank them and ask how you can help them \\n\\n         ### ask_for list: {ask_for}\"), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I need to gather some information from you. Could you please tell me your pick-up location?'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_for_info(ask_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input =\"from 271 Nguyen Van Linh to 460 Tran Dai Nghia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pick_up_location', 'destination_location', 'pick_up_time', 'flight_code']\n",
      "pick_up_location : from 271 Nguyen Van Linh to 460 Tran Dai Nghia\n",
      "name='Huy' number_phone='0917181880' pick_up_location='271 Đ. Nguyễn Văn Linh, Bình Thuận, Quận 7, Hồ Chí Minh, Vietnam' destination_location='460 P. Trần Đại Nghĩa, Đồng Tâm, Hoàng Mai, Hà Nội, Vietnam' pick_up_time='' flight_code='No Request'\n",
      "Field 'pick_up_time' is empty.\n"
     ]
    }
   ],
   "source": [
    "user_details, ask_for = filter_response(text_input, booking_details, ask_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Below is are some things to ask the user for in a coversation way. you should only ask one question at a time even if you don't get all the info         don't ask as a list! Don't greet the user! Don't say Hi.Explain you need to get some info. If the ask_for list is empty then thank them and ask how you can help them \\n\\n         ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I need to gather some information to assist you better. Could you please tell me your destination location?\n"
     ]
    }
   ],
   "source": [
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print('Everything gathered move to next phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['destination_location', 'pick_up_time']\n",
      "destination_location : 466 Tran Dai Nghia Da Nang\n",
      "name='Huy' number_phone='0917181880' pick_up_location='271 Đ. Nguyễn Văn Linh, Bình Thuận, Quận 7, Hồ Chí Minh, Vietnam' destination_location='466 Đ. Trần Đại Nghĩa, Điện Ngọc, Ngũ Hành Sơn, Đà Nẵng 550000, Vietnam' pick_up_time='' flight_code='No Request'\n",
      "Field 'pick_up_time' is empty.\n"
     ]
    }
   ],
   "source": [
    "text_input =\"466 Tran Dai Nghia Da Nang\"\n",
    "user_details, ask_for = filter_response(text_input, user_details, ask_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything gathered move to next phase\n"
     ]
    }
   ],
   "source": [
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print('Everything gathered move to next phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Below is are some things to ask the user for in a coversation way. you should only ask one question at a time even if you don't get all the info         don't ask as a list! Don't greet the user! Don't say Hi.Explain you need to get some info. If the ask_for list is empty then thank them and ask how you can help them \\n\\n         ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I need to gather some information from you. Could you please tell me your name?\n"
     ]
    }
   ],
   "source": [
    "if ask_for:\n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "else:\n",
    "    print('Everything gathered move to next phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field \n",
    "from langchain_core.tools import tool\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "class BookingCarDetails(BaseModel):\n",
    "    \"\"\"Details for the bookings car details\"\"\"\n",
    "    name: str = Field(\n",
    "        ...,\n",
    "        description=\"The name of the person booking the ride.This is optional if provided\",\n",
    "    )\n",
    "    number_phone: str = Field(\n",
    "        ...,\n",
    "        description=\"The phone number of the user.This is optional if provided\",\n",
    "    )\n",
    "    pick_up_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The location where the user will be picked up. This can be a full address or a specific location name.This is optional if provided\",\n",
    "    )\n",
    "    destination_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The destination location for the ride. This can be a full address or a specific location name.This is optional if provided\"\n",
    "    )\n",
    "    pick_up_time: str = Field(\n",
    "        ...,\n",
    "        description=\"The time the user intends to be picked up. No format keeps the text related to time..This is optional if provided\"\n",
    "    )\n",
    "    # @feild_validator('')\n",
    "\n",
    "\n",
    "def check_what_is_empty(user_personal_details):\n",
    "    ask_for = []\n",
    "    # Check if fields are empty\n",
    "    for field, value in user_personal_details.model_dump().items():\n",
    "        if value in [None, \"\", 0]:  # Add other 'empty' conditions if needed\n",
    "            print(f\"Field '{field}' is empty.\")\n",
    "            ask_for.append(field)\n",
    "    return ask_for\n",
    "\n",
    "\n",
    "def add_non_empty_details(current_details: BookingCarDetails, new_details: BookingCarDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.model_dump().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.model_copy(update=non_empty_details)\n",
    "    return updated_details\n",
    "\n",
    "\n",
    "def ask_for_info(ask_list: list):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \n",
    "        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\n",
    "        ### ask_for list: {ask_for}\"\"\"\n",
    "    )\n",
    "\n",
    "    info_gathering_chain = first_prompt | llm | StrOutputParser()\n",
    "    ai_chat = info_gathering_chain.invoke({\"ask_for\": ask_list})\n",
    "    print(first_prompt)\n",
    "    return ai_chat\n",
    "def filter_response(text_input, user_details ):\n",
    "    chain = llm.with_structured_output(BookingCarDetails)\n",
    "    res = chain.invoke(text_input)\n",
    "    # add filtered info to the\n",
    "    user_details = add_non_empty_details(user_details,res)\n",
    "    print(user_details)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    return user_details, ask_for\n",
    "def ask_confirm_info(booking_details: BookingCarDetails):\n",
    "    # booking_details.\n",
    "    message = (\n",
    "        f\"Please confirm your ride details:\\n\"\n",
    "        f\"- Pickup Location: {booking_details.pick_up_location}\\n\"\n",
    "        f\"- Destination: {booking_details.destination_location}\\n\"\n",
    "        f\"- Pickup Time: {booking_details.pick_up_time}\\n\"\n",
    "        f\"- Name: {booking_details.name}\\n\"\n",
    "        f\"- Contact Number: {booking_details.number_phone}\\n\"\n",
    "    )\n",
    "    print(message)\n",
    "@tool\n",
    "def get_booking_details(input_text):\n",
    "    \"\"\" This is function to get information for booking\"\"\"\n",
    "    chain = llm.with_structured_output(BookingCarDetails)\n",
    "    response_text = \"i want to book a car to 271 Nguyen Van Linh, Da Nang from 460 Tran Dai Nghia, Da Nang at 9 tomorrow \"\n",
    "    response = chain.invoke(input_text)\n",
    "    booking_details = BookingCarDetails(\n",
    "        name=\"\", number_phone=\"\", pick_up_location=\"\", destination_location=\"\", pick_up_time=\"\"\n",
    "    )\n",
    "    booking_details = add_non_empty_details(booking_details, response)\n",
    "    \n",
    "    ask_for = check_what_is_empty(booking_details)\n",
    "    \n",
    "    ai_response = ask_for_info(ask_for)\n",
    "    print(ai_response)\n",
    "    text_input = input()\n",
    "    user_details, ask_for = filter_response(text_input, booking_details)\n",
    "    while ask_for:  \n",
    "        ai_response = ask_for_info(ask_for)\n",
    "        input\n",
    "        print(ai_response)\n",
    "        text_input = input()\n",
    "        user_details, ask_for = filter_response(text_input, user_details)\n",
    "        print(ask_for)\n",
    "    \n",
    "    ask_confirm_info(user_details)\n",
    "    return \n",
    "    \n",
    "        \n",
    "\n",
    "    # # Validate locations\n",
    "    # validation_errors = validate_locations(booking_details)\n",
    "    # if validation_errors:\n",
    "    #     print(f\"Validation errors: {validation_errors}\")\n",
    "    # else:\n",
    "    #     print(\"All details are valid.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_booking_details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi want to book a car to 271 Nguyen Van Linh, Da Nang from 460 Tran Dai Nghia, Da Nang at 9 tomorrow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# response = chain.invoke(response_text)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mget_booking_details\u001b[49m(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_booking_details' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# chain = llm.with_structured_output(BookingCarDetails)\n",
    "response = \"i want to book a car to 271 Nguyen Van Linh, Da Nang from 460 Tran Dai Nghia, Da Nang at 9 tomorrow \"\n",
    "# response = chain.invoke(response_text)\n",
    "\n",
    "get_booking_details(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_booking_details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# system_prompt = ChatPromptTemplate.from_messages(\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     [\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#         (\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     12\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    You are a very powerful assistant. Be polite, clear, and understandable.\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m    If users ask general questions, answer them helpfully. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_react_agent(\u001b[43mllm\u001b[49m, tools \u001b[38;5;241m=\u001b[39m [get_booking_details] , state_modifier\u001b[38;5;241m=\u001b[39msystem_prompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# system_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are very powerful assistant, but don't know current events\",\n",
    "#         ),\n",
    "#         (\"user\", \"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "system_prompt = \"\"\"\n",
    "    You are a very powerful assistant. Be polite, clear, and understandable.\n",
    "    If users ask general questions, answer them helpfully. \n",
    "    If users want to book a ride, call the 'get_booking_details' function to gather booking information.\n",
    "    Please confirm information after done 'get_booking_details' .\n",
    "    If user confirm, call the 'get_quotes' ,else thank you user\n",
    "    \n",
    "    \"\"\"\n",
    "agent_executor = create_react_agent(llm, tools = [get_booking_details] , state_modifier=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "my name is Huy,I want to book a car from 271 Nguyen Van Linh to 466 NGuyen van linh da nang at now\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_booking_details (call_u4Kd8HniyFL3uqhfphrCoolg)\n",
      " Call ID: call_u4Kd8HniyFL3uqhfphrCoolg\n",
      "  Args:\n",
      "    input_text: I want to book a car from 271 Nguyen Van Linh to 466 Nguyen Van Linh Da Nang at now.\n",
      "Field 'name' is empty.\n",
      "Field 'number_phone' is empty.\n",
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \\n        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\\n        ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I'm gathering some information to help you better. Could you please provide your name?\n",
      "name='John Doe' number_phone='123-456-7890' pick_up_location='123 Main St, Springfield' destination_location='456 Elm St, Springfield' pick_up_time='3:00 PM'\n",
      "Please confirm your ride details:\n",
      "- Pickup Location: 123 Main St, Springfield\n",
      "- Destination: 456 Elm St, Springfield\n",
      "- Pickup Time: 3:00 PM\n",
      "- Name: John Doe\n",
      "- Contact Number: 123-456-7890\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_booking_details\n",
      "\n",
      "null\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems there was an issue with processing your booking request. However, I can help you with the details. \n",
      "\n",
      "You want to book a car from **271 Nguyen Van Linh** to **466 Nguyen Van Linh, Da Nang** right now. \n",
      "\n",
      "Please confirm if this is correct or provide any additional details you would like to include!\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"my name is Huy,I want to book a car from 271 Nguyen Van Linh to 466 NGuyen van linh da nang at now\")]}\n",
    "for s in agent_executor.stream(inputs, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icecream\n",
      "  Downloading icecream-2.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\os\\appdata\\roaming\\python\\python312\\site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in c:\\users\\os\\appdata\\roaming\\python\\python312\\site-packages (from icecream) (2.18.0)\n",
      "Collecting executing>=2.1.0 (from icecream)\n",
      "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\users\\os\\appdata\\roaming\\python\\python312\\site-packages (from icecream) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\os\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
      "Downloading icecream-2.1.4-py3-none-any.whl (14 kB)\n",
      "Using cached executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: executing, icecream\n",
      "  Attempting uninstall: executing\n",
      "    Found existing installation: executing 2.0.1\n",
      "    Uninstalling executing-2.0.1:\n",
      "      Successfully uninstalled executing-2.0.1\n",
      "Successfully installed executing-2.1.0 icecream-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"messages\": [(\"user\", task_formatted)]}\n",
    "    )\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PlanExecute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, START\n\u001b[1;32m----> 3\u001b[0m workflow \u001b[38;5;241m=\u001b[39m StateGraph(\u001b[43mPlanExecute\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Add the plan node\u001b[39;00m\n\u001b[0;32m      6\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplanner\u001b[39m\u001b[38;5;124m\"\u001b[39m, plan_step)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PlanExecute' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"agent\", END],\n",
    ")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'API'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Command, interrupt\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyMessage\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAPI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbooking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BookingAPI\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAPI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeoCoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoCodingAPI\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAPI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgetKey\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OAuthClient\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'API'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Annotated, Literal , List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field , field_validator, ValidationInfo\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph.message import AnyMessage\n",
    "from API.booking import BookingAPI\n",
    "from API.geoCoding import GeoCodingAPI\n",
    "from API.getKey import OAuthClient\n",
    "from API.getQuotes import QuotesAPI\n",
    "from API.is_Airport import IsAirport\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "jupiterAPI = os.getenv('JUPITER_API')\n",
    "quoteAPI = str(jupiterAPI) + \"/demand/v1/quotes\"\n",
    "bookingsAPI  = str(jupiterAPI) + '/demand/v1/bookings'\n",
    "\n",
    "class BookingCarDetails(BaseModel):\n",
    "    \"\"\"Details for the bookings car details\"\"\"\n",
    "    name: str = Field(\n",
    "        ...,\n",
    "        description=\"The name of the person booking the ride.This is optional if provided\",\n",
    "    )\n",
    "    number_phone: str = Field(\n",
    "        ...,\n",
    "        description=\"The phone number of the user.This is optional if provided\",\n",
    "    )\n",
    "    pick_up_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The location where the user will be picked up. This can be a full address or a specific location name.This is optional if provided\",\n",
    "    )\n",
    "    destination_location: str = Field(\n",
    "        ...,\n",
    "        description=\"The destination location for the ride. This can be a full address or a specific location name.This is optional if provided\"\n",
    "    )\n",
    "    pick_up_time: str = Field(\n",
    "        ...,\n",
    "        description=\"The time the user intends to be picked up. No format keeps the text related to time..This is optional if provided\"\n",
    "    )\n",
    "    @field_validator('pick_up_location')\n",
    "    @classmethod\n",
    "    def validate_pickup(cls, value:str):\n",
    "        geoCodingAPI = GeoCodingAPI()\n",
    "        if value == '':\n",
    "            return ''\n",
    "        else :\n",
    "            geoCoding_pickup = geoCodingAPI.get_geocoding(value)\n",
    "            if geoCoding_pickup[\"status\"] == \"OK\" :\n",
    "                return geoCoding_pickup['results'][0]['formatted_address']\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid pick-up location: {value}\")\n",
    "    @field_validator('destination_location')\n",
    "    @classmethod\n",
    "    def validate_destination(cls, value : str, info: ValidationInfo):\n",
    "        geoCodingAPI = GeoCodingAPI()\n",
    "        \n",
    "        # print (geoCoding_destination['results'][0]['formatted_address'])\n",
    "        if value == '':\n",
    "            return ''\n",
    "        else :\n",
    "            geoCoding_destination = geoCodingAPI.get_geocoding(value)\n",
    "            if geoCoding_destination[\"status\"] == \"OK\":\n",
    "                if geoCoding_destination['results'][0]['formatted_address'] == info.data['pick_up_location']:\n",
    "                    raise ValueError(f\"Invalid destination location: {value}\")\n",
    "                else:\n",
    "                    return geoCoding_destination['results'][0]['formatted_address']\n",
    "            else:\n",
    "            \n",
    "                raise ValueError(f\"Invalid destination location: {value}\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    # quote_id: str\n",
    "    # booking_info: BookingCarDetails\n",
    "    \n",
    "def check_what_is_empty(user_personal_details):\n",
    "    ask_for = []\n",
    "    # Check if fields are empty\n",
    "    for field, value in user_personal_details.model_dump().items():\n",
    "        if value in [None, \"\", 0]:  # Add other 'empty' conditions if needed\n",
    "            print(f\"Field '{field}' is empty.\")\n",
    "            ask_for.append(field)\n",
    "    return ask_for\n",
    "\n",
    "\n",
    "def add_non_empty_details(current_details: BookingCarDetails, new_details: BookingCarDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.model_dump().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.model_copy(update=non_empty_details)\n",
    "    ic(updated_details)\n",
    "    return updated_details\n",
    "\n",
    "def ask_for_info(ask_list: list):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \n",
    "        Explain you're gathering info to help.\n",
    "        ### ask_for list: {ask_for}\"\"\"\n",
    "    )\n",
    "\n",
    "    info_gathering_chain = first_prompt | llm | StrOutputParser()\n",
    "    ai_chat = info_gathering_chain.invoke({\"ask_for\": ask_list})\n",
    "    print(first_prompt)\n",
    "    return ai_chat\n",
    "def filter_response(text_input, user_details : BookingCarDetails ):\n",
    "    chain = llm.with_structured_output(BookingCarDetails)\n",
    "    res = chain.invoke(text_input)\n",
    "    user_details = add_non_empty_details(user_details,res)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    return user_details, ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_info_field(fields : List[str], booking_details : BookingCarDetails):\n",
    "    for field in fields:\n",
    "        ai_response = ask_for_info([field])\n",
    "        print(ai_response)\n",
    "        text_input = input()\n",
    "        user_details, ask_for = filter_response(text_input, booking_details)\n",
    "\n",
    "    print(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_booking_details():\n",
    "    \"\"\" Call function to get the details for a booking from user\"\"\"\n",
    "    chain = llm.with_structured_output(BookingCarDetails)\n",
    "    msg = 'i want to book a car to 271 Nguyen Van Linh, Da Nang from 460 Tran Dai Nghia, Da Nang at 9 tomorrow, my name is Huy call me 0917181880'\n",
    "    response =chain.invoke(msg)\n",
    "    booking_details = BookingCarDetails(\n",
    "        name=\"\", number_phone=\"\", pick_up_location=\"\", destination_location=\"\", pick_up_time=\"\"\n",
    "    )\n",
    "    user_details= add_non_empty_details(booking_details, response)\n",
    "    ask_for = check_what_is_empty(user_details)\n",
    "    while ask_for:  \n",
    "        ai_response = ask_for_info(ask_for)\n",
    "        print(ai_response)\n",
    "        text_input = input()\n",
    "        user_details, ask_for = filter_response(text_input, user_details)\n",
    "        print(ask_for)\n",
    "    return user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| updated_details: BookingCarDetails(name='Huy', number_phone='0917181880', pick_up_location='460 Đường Trần Đại Nghĩa, Hoà Hải, Ngũ Hành Sơn, Đà Nẵng 550000, Vietnam', destination_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='9 tomorrow')\n"
     ]
    }
   ],
   "source": [
    "user_details = get_booking_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for BookingCarDetails\nflight_code\n  Field required [type=missing, input_value={'name': 'hUY', 'number_p..., 'pick_up_time': 'now'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_details \u001b[38;5;241m=\u001b[39m \u001b[43mBookingCarDetails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhUY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnumber_phone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpick_up_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m271 Nguyen Van Linh, Da Nang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m01 Nguyen Van Linh, Da Nang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpick_up_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for BookingCarDetails\nflight_code\n  Field required [type=missing, input_value={'name': 'hUY', 'number_p..., 'pick_up_time': 'now'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "user_details = BookingCarDetails(name=\"hUY\",number_phone=\"\",pick_up_location=\"271 Nguyen Van Linh, Da Nang\", destination_location=\"01 Nguyen Van Linh, Da Nang\", pick_up_time=\"now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \\n        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\\n        ### ask_for list: {ask_for}\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "ai_response = ask_for_info(['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm gathering some information to help you better. Could you please tell me your name?\n"
     ]
    }
   ],
   "source": [
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = update_details(user_details, 'name', 'khoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm.with_structured_output(BookingCarDetails)\n",
    "msg = 'bye'\n",
    "response =chain.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookingCarDetails(name='John Doe', number_phone='123-456-7890', pick_up_location='123 Main St, Springfield, MA 01105, USA', destination_location='456 Elm St, West Springfield, MA 01089, USA', pick_up_time='3:00 PM')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_non_empty_details(current_details: BookingCarDetails, new_details: BookingCarDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.model_dump().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.model_copy(update=non_empty_details)\n",
    "    return updated_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| updated_details: BookingCarDetails(name='Khoa', number_phone='', pick_up_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='HAGL Plaza Danang, 01 Đ. Nguyễn Văn Linh, Nam Dương, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='now')\n",
      "ic| non_empty_details: {'name': 'Khoa'}\n"
     ]
    }
   ],
   "source": [
    "user_details = add_non_empty_details( user_details,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| updated_details: BookingCarDetails(name='khoa', number_phone='0917181880', pick_up_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='HAGL Plaza Danang, 01 Đ. Nguyễn Văn Linh, Nam Dương, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='now')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "details, ask_for = filter_response(\"0917181880\", user_details )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Khoa' number_phone='' pick_up_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam' destination_location='HAGL Plaza Danang, 01 Đ. Nguyễn Văn Linh, Nam Dương, Hải Châu, Đà Nẵng 550000, Vietnam' pick_up_time='now'\n"
     ]
    }
   ],
   "source": [
    "print(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \\n        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\\n        ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I'm gathering some information to help you better. Could you please tell me your destination location?\n",
      "name='Nguyen Van Linh' number_phone='0917181880' pick_up_location='1 Đ. Nguyễn Văn Linh, Phường 7, Bình Chánh, Hồ Chí Minh, Vietnam' destination_location={'271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam'} pick_up_time='9 tomorrow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\pydantic\\main.py:347: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `set` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "change_info_field(['destination_location'],user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookingCarDetails(name='hUY', number_phone='', pick_up_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='HAGL Plaza Danang, 01 Đ. Nguyễn Văn Linh, Nam Dương, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='now')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_info_field(fields : List[str], booking_details : BookingCarDetails):\n",
    "    for field in fields:\n",
    "        ai_response = ask_for_info([field])\n",
    "        print(ai_response)\n",
    "        text_input = input()\n",
    "        chain = llm.with_structured_output(BookingCarDetails)\n",
    "        response =chain.invoke(text_input)\n",
    "        booking_details = add_non_empty_details( booking_details,response)\n",
    "    print(booking_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \\n        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\\n        ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I'm gathering some information to help you better. Could you please tell me your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| updated_details: BookingCarDetails(name='Khoa', number_phone='', pick_up_location='271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='HAGL Plaza Danang, 01 Đ. Nguyễn Văn Linh, Nam Dương, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='now')\n",
      "ic| non_empty_details: {'name': 'Khoa'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['ask_for'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ask_for'], input_types={}, partial_variables={}, template=\"Ask one question at a time, even if you don't get all the info. Don't list the questions or greet the user. \\n        Explain you're gathering info to help. If 'ask_for' is empty, thank the user and ask how you can assist next.\\n        ### ask_for list: {ask_for}\"), additional_kwargs={})]\n",
      "I'm gathering some information to help you better. Could you please provide the pick-up location?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| updated_details: BookingCarDetails(name='John Doe', number_phone='1234567890', pick_up_location='5 Đ. Nguyễn Văn Linh, Bình Hiên, Hải Châu, Đà Nẵng 550000, Vietnam', destination_location='Danang International Airport, Đ. Nguyễn Văn Linh, Hải Châu, Đà Nẵng 550000, Vietnam', pick_up_time='2023-10-15 10:00 AM')\n",
      "ic| non_empty_details: {'destination_location': 'Danang International Airport, Đ. Nguyễn Văn Linh, '\n",
      "                                                'Hải Châu, Đà Nẵng 550000, Vietnam',\n",
      "                        'name': 'John Doe',\n",
      "                        'number_phone': '1234567890',\n",
      "                        'pick_up_location': '5 Đ. Nguyễn Văn Linh, Bình Hiên, Hải Châu, Đà Nẵng '\n",
      "                                            '550000, Vietnam',\n",
      "                        'pick_up_time': '2023-10-15 10:00 AM'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' number_phone='1234567890' pick_up_location='5 Đ. Nguyễn Văn Linh, Bình Hiên, Hải Châu, Đà Nẵng 550000, Vietnam' destination_location='Danang International Airport, Đ. Nguyễn Văn Linh, Hải Châu, Đà Nẵng 550000, Vietnam' pick_up_time='2023-10-15 10:00 AM'\n"
     ]
    }
   ],
   "source": [
    "change_info_field(['name', 'pick_up_location'], user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Optional \n",
    "from pydantic import BaseModel, Field \n",
    "\n",
    "class ChangeRequest(BaseModel):\n",
    "    changes: Dict[str, Optional[str]] = Field(\n",
    "        ...,\n",
    "        description=\"List of changes\",\n",
    "    )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:417: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'ChangeRequest': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'changes' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(ChangeRequest)\n\u001b[1;32m----> 2\u001b[0m reponse \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi want cancel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3014\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3012\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3014\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3016\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    283\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:784\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 784\u001b[0m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers:\n\u001b[0;32m    786\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:418\u001b[0m, in \u001b[0;36m_handle_openai_bad_request\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m    410\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid schema for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms structured output feature, which is the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault method for `with_structured_output` as of langchain-openai==0.3. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/structured-outputs#supported-schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     )\n\u001b[0;32m    417\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:782\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 782\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m     _handle_openai_bad_request(e)\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py:160\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[1;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[0;32m    155\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[0;32m    156\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[0;32m    157\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    158\u001b[0m     )\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OS\\anaconda3\\envs\\env_langchain\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'ChangeRequest': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'changes' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "chain = llm.with_structured_output(ChangeRequest)\n",
    "reponse = chain.invoke(\"i want cancel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FieldChange(field_name='name', new_value='Khoa'), FieldChange(field_name='pick_up_time', new_value=None)]\n",
      "<class 'str'>\n",
      "[FieldChange(field_name='pick_up_time', new_value=None)]\n",
      "name='Khoa' number_phone='' pick_up_location='' destination_location='' pick_up_time='' flight_code=''\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Dict, Optional , Literal , List\n",
    "\n",
    "class FieldChange(BaseModel):\n",
    "    field_name: str = Field(\n",
    "        ..., description= \"The name of the field to be changed. Must be one of: 'name', 'number_phone', 'pick_up_location', 'destination_location', 'pick_up_time', 'flight_code'.\"\n",
    "    )\n",
    "    new_value: Optional[str] = Field(None, description=\"The new value of the field,Return 'None' if unspecified.\")\n",
    "\n",
    "class ChangeRequest(BaseModel):\n",
    "    changes: List[FieldChange] = Field(..., description=\"A list of requested changes, each containing a field name and its new value.\")\n",
    "\n",
    "\n",
    "\n",
    "chain = llm.with_structured_output(ChangeRequest)\n",
    "response = chain.invoke(\"i want to change name to Khoa and pick up time \")\n",
    "print(response.changes)\n",
    "new_details = BookingCarDetails(name=\"\", number_phone=\"\", pick_up_location=\"\", destination_location=\"\", pick_up_time=\"\", flight_code=\"\")\n",
    "processed_fields = []\n",
    "for change in response.changes:\n",
    "    if change.field_name not in [None, \"None\"] and change.field_name in [\"name\", \"number_phone\", \"pick_up_location\", \"destination_location\", \"pick_up_time\", \"flight_code\"]:\n",
    "        if change.new_value not in [None,\"None\"] :\n",
    "            setattr(new_details, change.field_name, change.new_value)\n",
    "            processed_fields.append(change)\n",
    "            \n",
    "        # else:\n",
    "        #     print(f\"{change.field_name}\")\n",
    "        #     user=input()\n",
    "        #     chain = llm.with_structured_output(BookingCarDetails)\n",
    "        #     messages = f\"{change.field_name} : {user}\"\n",
    "        #     print(messages)\n",
    "        #     res = chain.invoke(messages)\n",
    "        #     # add filtered info to the\n",
    "        #     new_details = add_non_empty_details(new_details,res)\n",
    "response.changes = [change for change in response.changes if change not in processed_fields]\n",
    "if response.changes : \n",
    "    print(type(response.changes[0].field_name))\n",
    "    print(response.changes)\n",
    "    print(new_details)\n",
    "else:\n",
    "    print(response.changes)\n",
    "    \n",
    "    print(new_details)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_for_duckling(text, dims):\n",
    "    url = 'http://localhost:8000/parse'\n",
    "    data = {\n",
    "        'locale': 'en_US',\n",
    "        'text': text,\n",
    "        'dims': dims,\n",
    "        'tz': \"Asia/Ho_Chi_Minh\"\n",
    "    }\n",
    "    response = requests.post(url, data=data)\n",
    "    if response.status_code == 200:\n",
    "        json_response = response.json()\n",
    "        # value = json_response[0]['value']['value']\n",
    "        return json_response\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"time\"]\n",
    "data = getData_for_duckling(message,dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data[0]['value']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from booking_agent.api.geoCoding import GeoCodingAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "271 Đ. Nguyễn Văn Linh, Vĩnh Trung, Hải Châu, Đà Nẵng 550000, Vietnam\n"
     ]
    }
   ],
   "source": [
    "geoCodingAPI = GeoCodingAPI()\n",
    "geoCoding_pickup = \"\"\n",
    "geoCoding_pickup = geoCodingAPI.get_geocoding(\"271 Nguyen Van Linh, Da Nang\")\n",
    "if geoCoding_pickup[\"status\"] == \"OK\" :\n",
    "    print(type(geoCoding_pickup))\n",
    "    print(geoCoding_pickup['results'][0]['formatted_address'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
